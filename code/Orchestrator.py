"""
Biomedical Research Analysis System
 - Collect research from PubMed/PMC 
 - Analyze and summarize research using OpenAI LLM
 _/ Integrate results
"""
import os
import json
import asyncio
import aiohttp
from typing import List, Optional
from datetime import datetime
from Fetch_data import ResearchPaper, PubMedClient, BiomedicalDatabaseClient, PubMedSearchParams
from LLM import LLMAnalyzer, OpenAIAnalyzer
import logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


# ============================================================================
# Main Orchestrator
# ============================================================================

class ResearchAnalyzer:
    """Main orchestrator for biomedical research collection and analysis"""
    
    def __init__(
        self,
        database_client: BiomedicalDatabaseClient,
        llm_analyzer: LLMAnalyzer
    ):
        """
        Initialize analyzer
        
        Args:
            database_client: Client for biomedical database
            llm_analyzer: LLM analyzer for relevance assessment
        """
        self.database_client = database_client
        self.llm_analyzer = llm_analyzer
        self.llm_query = None
        
    def search_papers(self, 
                      LLMaugSearch: bool,
                      user_query: Optional[str] = None,
                      search_params: Optional[PubMedSearchParams] = None,
                      max_results: int = 100,
                      advanced_options: dict = None) -> List[str]:
        """
        Search for papers in the database and return papers' brief info
        
        Args:
            LLMaugSearch: If True, use llm-generated search parameters
            user_query: Simple query to search in database
            search_params: Advanced search parameters object (set by user artificially)
            
        Returns:
            search parameters generated by LLM, List of PubMed IDs (PMIDs), brief info of papers
        """
        # Step 1: Search for query, get list of PMIDs
        if user_query:
            self.user_query=user_query
            logger.info(f"Use LLM to augment user's query: {user_query}")
            try:
                paper_ids = self.database_client.search(query=user_query, max_results=max_results, year_range=advanced_options["year_range"], llm_search=False)
                if LLMaugSearch:
                    paper_ids = paper_ids[:int(max_results*0.8) if len(paper_ids) > int(max_results*0.8) else len(paper_ids)]
                    self.llm_query = self.llm_analyzer.generate_pubmed_query(user_query, advanced_options)
                    paper_ids_llm = self.database_client.search(query=self.llm_query, max_results=max_results, llm_search=True)
                    for pid in paper_ids_llm:
                        if pid not in paper_ids:
                            paper_ids.append(pid)
                        if len(paper_ids) >= max_results:
                            break
            except Exception as e: # Fallback to direct search if LLM fails
                logger.error(f"Query generation failed: {e}")
                logger.info(f"Use user's query directly without LLM augmentation.")
                paper_ids = self.database_client.search(query=user_query, max_results=max_results, llm_search=False)
        
        elif search_params:
            logger.info(f"Use advanced search parameters")
            paper_ids = self.database_client.search(search_params=search_params, llm_search=False)
        
        # Step 2: Get brief info for each paper
        if not paper_ids:
            logger.warning("No papers found")
            return self.llm_query
        else:
            logger.info(f"Found {len(paper_ids)} papers")
            brief_papers = self.database_client.fetch_brief(paper_ids)

        return paper_ids, brief_papers, self.llm_query
    
    def extract_abstracts(self, papers: List) -> List[ResearchPaper]:
        """
        Fetch abstracts for papers by PubMed ID
        
        Args:
            papers: List of papers to analyze directly, [{paper1}, {paper2}, ...]
        Returns:
            list of ResearchPaper objects with abstracts
        """
        papers_ab = []
        for paper in papers:
            pmid=paper.get('pmid', '')
            # Create ResearchPaper object
            paper_ab = ResearchPaper(
                pmid=pmid,
                journal=paper.get('Journal', ''),
                title=paper.get('Title', ''),
                abstract=self.database_client._fetch_abstract(pmid)
            )
            papers_ab.append(paper_ab)
        return papers_ab
    
    async def async_extract_abstracts(self, pmids: List[str], extract_bar, max_concurrent=10) -> List[str]:
        """Fetch multiple PMIDs concurrently using multiple API accounts"""
        semaphore = asyncio.Semaphore(max_concurrent)
        async with aiohttp.ClientSession() as session:
            tasks = []
            progress = [0, len(pmids)]
            for i, pmid in enumerate(pmids):
                tasks.append(asyncio.create_task(
                    self.database_client._fetch_one(session, pmid, semaphore, extract_bar, progress)))

            responses = await asyncio.gather(*tasks)
        # Create ResearchPaper object
        papers = []
        for pmid, paper_ab in zip(pmids, responses):
            if paper_ab is None:
                logger.warning(f"Failed to fetch abstract for {pmid}")
                continue
            paper = ResearchPaper(
                pmid=pmid,
                abstract=paper_ab
            )
            papers.append(paper)
        return papers

        
    async def async_analyze_papers(self, papers: List[ResearchPaper], relevance_theme: str, analyze_bar, batch_size: int = 10) -> List[ResearchPaper]:
        """
        Research papers provided directly, analyze them in batches concurrently.

        Args:
            papers: List of ResearchPaper objects to analyze directly
            relevance_theme: Theme to check relevance against
        Returns:
            List of analyzed ResearchPaper objects
        """
        logger.info("=" * 80)
        logger.info(f"Starting analysis pipeline")
        logger.info(f"Relevance theme: {relevance_theme}")

        tasks = []
        progress = [0, (len(papers) + batch_size - 1) // batch_size]
        for i in range(0, len(papers), batch_size):
            batch = papers[i:i+batch_size]
            tasks.append(self.llm_analyzer.async_analyze(batch, relevance_theme, analyze_bar, progress))

        analyzed_papers = await asyncio.gather(*tasks)
        all_analyzed_papers = [item for batch in analyzed_papers for item in batch]
        return all_analyzed_papers
    
    def analyze_paper_ids(
        self,
        paper_ids: List[str]=None,
        relevance_theme: str = "",
        filter_relevant_only: bool = False
    ) -> List[ResearchPaper]:
        """
        Search, collect, and analyze research papers(only paper IDs are provided, fetch abstracts then analyze one by one)

        Args:
            paper_ids: List of PubMed IDs
            relevance_theme: Theme to check relevance against
            filter_relevant_only: If True, return only relevant papers
        Returns:
            List of analyzed ResearchPaper objects
        """
        logger.info("=" * 80)
        logger.info(f"Starting analysis pipeline")
        logger.info(f"Relevance theme: {relevance_theme}")
        if paper_ids:
            # Step 1: Fetch detailed information
            papers = self.database_client.fetch_details(paper_ids)
            if not papers:
                logger.warning("No paper details retrieved")
                return []
            
            # Step 2: Analyze each paper with LLM
            for index, paper in enumerate(papers, 1):
                if paper.abstract:
                    logger.info(f"Analyzing {index}/{len(papers)} papers...")
                    try:
                        # LLM analyze and update the result to paper
                        self.llm_analyzer.analyze(paper, relevance_theme) 
                        # Incrementally save each analysis result to JSON
                        self._update_analysis_to_json(paper, output_file="Analysis.json")
                    except Exception as e:
                        logger.error(f"Analysis failed for paper {paper.pmid}: {e}")
                        break
                    logger.info(f"Analysis complete. Relevant: {paper.is_relevant}")
                else:
                    logger.warning(f"Skipping analysis for {paper.pmid} - no abstract")
            
        # Step 3: Filter if requested
        if filter_relevant_only:
            papers = [p for p in papers if p.is_relevant]
            self._save_papers_to_json(papers, output_file="relevant_papers.json")
        
        return papers
    
    def report(self, query: str, contents: List[str]) -> str:
        """Generate a summary report for the given theme and content.

        Args:
            query: User's query
            contents: List of papers' pmid+summary, [f"{pmid}. {summary}",...]
        Returns:
            Summary report
        """
        batch_size = 30
        if not contents:
            logger.warning("No papers to generate report")
            return ""
        elif len(contents) > batch_size:
            logger.warning(f"Split {len(contents)} papers into {len(contents) // batch_size + 1} batches to generate report.")
            for i in range(0, len(contents), batch_size):
                logger.warning(f"Generating report for {i} - {i+batch_size if i+batch_size < len(contents) else len(contents)} papers...")
                contents_text = "\n\n".join([str(c).strip() for c in contents[i:i+batch_size] if c])
                if i == 0:
                    final_report = self.llm_analyzer.generate_report(query, contents_text)
                else:
                    batch_report = self.llm_analyzer.generate_report(query, contents_text)
                    final_report = self.llm_analyzer.integrate_reports(query, final_report, batch_report)
        else:
            logger.warning(f"Generate report for {len(contents)} papers...")
            contents_text = "\n\n".join([str(c).strip() for c in contents if c])
            final_report = self.llm_analyzer.generate_report(query, contents_text)
        return final_report

    def _save_papers_to_json(self, papers: List[ResearchPaper], output_file: str = "papers.json"):
        """
        Save analyzed papers into a structured JSON file.

        Args:
            papers (List): List of ResearchPaper-like objects
            output_file (str): Output JSON file name
        """
        metadata = {
            "project": "Biomedical Research Analysis",
            "created": datetime.now().isoformat(),
            'total_papers': sum(1 for p in papers if p),
            'relevant_papers': sum(1 for p in papers if p.is_relevant)
        }

        data = []
        for paper in papers:
            paper_dict = {
                "title": paper.title,
                "authors": paper.authors,
                "publication_date": paper.publication_date,
                "article_types": paper.article_types,
                "journal": paper.journal,
                "pmc_cited": paper.pmc_cited,
                "doi": paper.doi if hasattr(paper, "doi") else None,
                "pmc_id": paper.pmc_id if hasattr(paper, "pmc_id") else None,
                "pmid": paper.pmid,
                "pubmed_url": paper.pubmed_url if hasattr(paper, "pubmed_url") else None,
                "abstract": paper.abstract if hasattr(paper, "abstract") else None,
                "is_relevant": paper.is_relevant,
                "relevance_reason": paper.relevance_reason,
                "simplified_summary": paper.simplified_summary,
                "category": paper.category,
                "analysis_timestamp": paper.analysis_timestamp
            }
            data.append(paper_dict)

        output = {
            "metadata": metadata,
            "papers": data
        }
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(output, f, indent=4, ensure_ascii=False)

        logger.info(f"{len(data)} Papers exported to {output_file}")

    def _update_analysis_to_json(self, new_paper: dict, output_file: str = "papers.json"):
        """
        Update or append a single paper's analysis result to a JSON file.
        """
        output_folder = "data"
        os.makedirs(output_folder, exist_ok=True)
        output_path = os.path.join(output_folder, output_file)

        # If output file not exist, create new file with metadata
        if not os.path.exists(output_path):
            data = {
                "metadata": {
                    "Description": "PubMed papers data with llm's analysis results",
                    "total_papers": 0,
                    "relevant_papers": 0,
                    "query_theme": self.user_query,
                    "search_params": self.llm_query,
                    "created": datetime.now().isoformat()
                },
                "papers": []
            }
        else:
            with open(output_path, "r", encoding="utf-8") as f:
                data = json.load(f)
        
        # Update metadata and data
        paper_dict = {
            "title": new_paper.title,
            "authors": new_paper.authors,
            "publication_date": new_paper.publication_date,
            "article_types": new_paper.article_types,
            "journal": new_paper.journal,
            "pmc_cited": new_paper.pmc_cited,
            "doi": new_paper.doi if hasattr(new_paper, "doi") else None,
            "pmc_id": new_paper.pmc_id if hasattr(new_paper, "pmc_id") else None,
            "pmid": new_paper.pmid,
            "pubmed_url": new_paper.pubmed_url if hasattr(new_paper, "pubmed_url") else None,
            "abstract": new_paper.abstract if hasattr(new_paper, "abstract") else None,
            "is_relevant": new_paper.is_relevant,
            "relevance_reason": new_paper.relevance_reason,
            "simplified_summary": new_paper.simplified_summary,
            "category": new_paper.category,
            "analysis_timestamp": new_paper.analysis_timestamp
        }
        data["papers"].append(paper_dict)
        data["metadata"]["total_papers"] = len(data["papers"])
        data["metadata"]["relevant_papers"] = sum(1 for p in data["papers"] if p.get("is_relevant"))

        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(data, f, indent=4, ensure_ascii=False)
        logger.info(f"{len(data['papers'])} Papers exported to {output_file}")

    def print_summary(self, papers: List[ResearchPaper]):
        """Print a summary of analysis results"""
        print("\n" + "=" * 80)
        print("ANALYSIS SUMMARY")
        print("=" * 80)
        
        relevant_papers = [p for p in papers if p.is_relevant]
        
        print(f"\nTotal papers analyzed: {len(papers)}")
        print(f"Relevant papers found: {len(relevant_papers)}")
        print(f"Relevance rate: {len(relevant_papers)/len(papers)*100:.1f}%")
        
        if relevant_papers:
            print("\n" + "-" * 80)
            print("RELEVANT PAPERS:")
            print("-" * 80)
            
            for i, paper in enumerate(relevant_papers, 1):
                print(f"\n[{i}] {paper.title}")
                print(f"    Authors: {', '.join(paper.authors[:3])}{'...' if len(paper.authors) > 3 else ''}")
                print(f"    Journal: {paper.journal} ({paper.publication_date})")
                print(f"    PMID: {paper.pmid}")
                if paper.pubmed_url:
                    print(f"    URL: {paper.pubmed_url}")
                print(f"\n    Reason: {paper.relevance_reason}")
                if paper.simplified_summary:
                    print(f"\n    Summary: {paper.simplified_summary}")
        
        print("\n" + "=" * 80)



# ============================================================================
# For Test
# ============================================================================

def main():
    # Initialize components
    pubmed_client = PubMedClient()
    openai_analyzer = OpenAIAnalyzer(api_key=os.getenv('EX_OPENAI_API_KEY'), model="gpt-4o-mini")
    # Create analyzer
    analyzer = ResearchAnalyzer(
        database_client=pubmed_client,
        llm_analyzer=openai_analyzer
    )

    # Example
    print("\n" + "="*80)
    relevance_theme="reducing blood lipids through pharmaceutical interventions"
    filter_relevant_only=False
    search_params = PubMedSearchParams(
        query="Human Hyperlipidemia",
        max_results=1000,
        
        ### Date filters - last 5 years
        date_from="2020/01/01",
        date_to="2025/12/31",
        date_type="pdat",  # Publication date
        
        ### Quality filters
        has_abstract=True,
        languages=["eng"],
        free_full_text=True,
        
        ### MeSH terms for precise medical concepts
        mesh_terms=["Hyperlipidemias", "Hypolipidemic Agents", "Lipoproteins", "Cholesterol", "Cardiovascular Diseases", "Atherosclerosis"],
        
        ### Publication filters
        publication_types=["Clinical Trial", "Randomized Controlled Trial", "Meta-Analysis"],
        article_types=["Journal Article"],
        journals=["The Lancet", "New England Journal of Medicine", "JAMA"],  # High-impact journals

        ### Species/Sex/Age specific filter
        species=["humans"],
        sex="female",
        age_groups=["Adult", "Middle Aged"],        

        ### Sort by (default is relevance)
        sort_by="pub_date"  # publication date(newest first)
    )
    paper_ids = analyzer.search_papers(search_params=search_params)
    papers = analyzer.analyze_research(
        paper_ids=paper_ids[:3],
        # search_query="hyperlipidemia treatment",
        relevance_theme=relevance_theme,
        filter_relevant_only=filter_relevant_only
    )
    analyzer.print_summary(papers)


    user_query = "the application of CRISPR gene editing in cancer treatment"
    llm_query, paper_ids = analyzer.search_papers(user_query=user_query)
    print(f"LLM generated PubMed query: {llm_query}")
    print(f"Found {len(paper_ids)} papers matching the query.")

    

if __name__ == "__main__":
    main()